{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47af52a7-7e46-4e7e-b455-ab1c30cf2c1c",
   "metadata": {},
   "source": [
    "# Trustworthy Location Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f3c521-90c8-4145-aa2a-a5d3ed47d2fc",
   "metadata": {},
   "source": [
    "- Team Name: Code Fellas\n",
    "- Team Members: Rayaan Nabi Ahmed Quraishi, Kunal Soni"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedac817-df9b-4453-989c-b6053baf57e1",
   "metadata": {},
   "source": [
    "## Part 1: Data Labelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac389336-73ac-4366-aca1-f8e6f078944e",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "87ea6e6e-c663-46a4-8e20-a64b0cfd57a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import pipeline\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d502d0-66bb-44a9-b9d2-77f7558dfdfa",
   "metadata": {},
   "source": [
    "### Data Loading and Eye-Balling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6ec9d3e1-8070-43a8-9465-b1553ee98aa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Set Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "48c4c2a1-f49c-45b9-9105-93dbf1ccb96e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_name</th>\n",
       "      <th>author_name</th>\n",
       "      <th>text</th>\n",
       "      <th>photo</th>\n",
       "      <th>rating</th>\n",
       "      <th>rating_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Haci'nin Yeri - Yigit Lokantasi</td>\n",
       "      <td>Gulsum Akar</td>\n",
       "      <td>We went to Marmaris with my wife for a holiday...</td>\n",
       "      <td>dataset/taste/hacinin_yeri_gulsum_akar.png</td>\n",
       "      <td>5</td>\n",
       "      <td>taste</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Haci'nin Yeri - Yigit Lokantasi</td>\n",
       "      <td>Oguzhan Cetin</td>\n",
       "      <td>During my holiday in Marmaris we ate here to f...</td>\n",
       "      <td>dataset/menu/hacinin_yeri_oguzhan_cetin.png</td>\n",
       "      <td>4</td>\n",
       "      <td>menu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Haci'nin Yeri - Yigit Lokantasi</td>\n",
       "      <td>Yasin Kuyu</td>\n",
       "      <td>Prices are very affordable. The menu in the ph...</td>\n",
       "      <td>dataset/outdoor_atmosphere/hacinin_yeri_yasin_...</td>\n",
       "      <td>3</td>\n",
       "      <td>outdoor_atmosphere</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Haci'nin Yeri - Yigit Lokantasi</td>\n",
       "      <td>Orhan Kapu</td>\n",
       "      <td>Turkey's cheapest artisan restaurant and its f...</td>\n",
       "      <td>dataset/indoor_atmosphere/hacinin_yeri_orhan_k...</td>\n",
       "      <td>5</td>\n",
       "      <td>indoor_atmosphere</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Haci'nin Yeri - Yigit Lokantasi</td>\n",
       "      <td>Ozgur Sati</td>\n",
       "      <td>I don't know what you will look for in terms o...</td>\n",
       "      <td>dataset/menu/hacinin_yeri_ozgur_sati.png</td>\n",
       "      <td>3</td>\n",
       "      <td>menu</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     business_name    author_name  \\\n",
       "0  Haci'nin Yeri - Yigit Lokantasi    Gulsum Akar   \n",
       "1  Haci'nin Yeri - Yigit Lokantasi  Oguzhan Cetin   \n",
       "2  Haci'nin Yeri - Yigit Lokantasi     Yasin Kuyu   \n",
       "3  Haci'nin Yeri - Yigit Lokantasi     Orhan Kapu   \n",
       "4  Haci'nin Yeri - Yigit Lokantasi     Ozgur Sati   \n",
       "\n",
       "                                                text  \\\n",
       "0  We went to Marmaris with my wife for a holiday...   \n",
       "1  During my holiday in Marmaris we ate here to f...   \n",
       "2  Prices are very affordable. The menu in the ph...   \n",
       "3  Turkey's cheapest artisan restaurant and its f...   \n",
       "4  I don't know what you will look for in terms o...   \n",
       "\n",
       "                                               photo  rating  \\\n",
       "0         dataset/taste/hacinin_yeri_gulsum_akar.png       5   \n",
       "1        dataset/menu/hacinin_yeri_oguzhan_cetin.png       4   \n",
       "2  dataset/outdoor_atmosphere/hacinin_yeri_yasin_...       3   \n",
       "3  dataset/indoor_atmosphere/hacinin_yeri_orhan_k...       5   \n",
       "4           dataset/menu/hacinin_yeri_ozgur_sati.png       3   \n",
       "\n",
       "      rating_category  \n",
       "0               taste  \n",
       "1                menu  \n",
       "2  outdoor_atmosphere  \n",
       "3   indoor_atmosphere  \n",
       "4                menu  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load Data\n",
    "data = pd.read_csv('reviews.csv',encoding='ISO-8859-1')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdb70fc-1acb-449b-a222-f164c3efcf40",
   "metadata": {},
   "source": [
    "The problem statement demands that: \n",
    "1. we determine the review quality with following characteristics: not a spam, not an advertisement, relevant to location, and without user rants.\n",
    "2. we enforce following policies: \"no advertisement or promotional content\",\"no irrelevant content\", and \"no rants or user complaints\"\n",
    "If we look at the data above, we do not have any of the characteristic above labelled for us. Nor we have the location tagged. Therefore, we take an approach in which we first tag the reviews with the following labels.\n",
    "    1. location (categorical): restaurant, home, shop, office etc. (depending upon the text in rating category)\n",
    "    2. spam (categorical): 1 if review is spam, 0 otherwise.\n",
    "    3. advertisement (categorical): 1 if review is advertisement, 0 otherwise.\n",
    "    4. irrelevant content (categorical): 1 if review is irrelevant to the location, 0 otherwise\n",
    "    5. rants (categorical): 1 if review is rant, 0 othewise\n",
    "    6. review quality score: a metric that fuses the information in 4 tags: spam, advertisement, irrelevancy, and rants. \n",
    "We use the generative AI tools for tagging all the labels which is roughly an unsupervised learning. Once the data is tagged, we split the data into training and testing sets and perform supervised learning using deep learning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ae814297-2376-4445-a28e-ed3dbb5096cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column Names: ['business_name', 'author_name', 'text', 'photo', 'rating', 'rating_category']\n",
      "Number of Rows: 1100\n"
     ]
    }
   ],
   "source": [
    "#Data Column Labels and Number of Rows\n",
    "columns = data.columns.tolist()\n",
    "print(\"Column Names:\",columns)\n",
    "print(\"Number of Rows:\",data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "80a056e9-b169-4fbd-a4ea-5b04995d9e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop Rows with NAs\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7fb76981-7043-4911-9c12-123cd6d50745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Rows: 1100\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Rows:\",data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e9632519-13fd-4ea5-8a51-125a92783dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text Preprocessing\n",
    "def preprocess_text(text):\n",
    "  text = str(text)#Convert everything to string\n",
    "  text = re.sub(r\"http\\S+\", \"\", text)   #Remove URLs\n",
    "  text = re.sub(r\"[^a-zA-Z\\s]\", \" \", text)  # Remove special characters and numbers\n",
    "  text = re.sub(r\"\\s+\", \" \", text)  # Remove extra whitespaces\n",
    "  text = text.lower()  # Convert to lowercase\n",
    "  text = text.strip() # Remove leading/trailing spaces\n",
    "  return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2901e3b4-ddd6-463f-bec3-0966c118c024",
   "metadata": {},
   "source": [
    "### Location Tagging (Prompt Engineering)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d3f2a9-3551-436d-8edb-e48920e26754",
   "metadata": {},
   "source": [
    "1. A quick glance at data suggests that business name can be in language such as Turkish. Therefore, it might be hard to extract location type from business name.\n",
    "2. However, location type can be extracted from text in rating_category. This could be highly contextual, so we prefer prompting to give context to LLM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "92545ccc-21d7-4982-9a3c-e8ac5464aa63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Location Tagging\n",
    "\n",
    "def tag_location(features):\n",
    "    \"\"\"\n",
    "    Classifies a location based on a list of features, ensuring a specific output\n",
    "    for a known input by using a highly-structured prompt with a Flan-T5 model.\n",
    "\n",
    "    Args:\n",
    "        features (list): A list of strings describing a place.\n",
    "\n",
    "    Returns:\n",
    "        str: The identified location category, or 'unidentified' if no match is found.\n",
    "    \"\"\"\n",
    "    # Initialize the text-to-text generation pipeline with the Flan-T5 model.\n",
    "    try:\n",
    "        generator = pipeline('text2text-generation', model='google/flan-t5-small')\n",
    "    except Exception as e:\n",
    "        return f\"Error loading model: {e}. Please ensure 'transformers' and 'torch' are installed.\"\n",
    "\n",
    "    # Define the specific categories the model should choose from.\n",
    "    categories = ['restaurant', 'office', 'home', 'park', 'hotel', 'shop']\n",
    "    \n",
    "    # Construct the highly-structured prompt as a variable.\n",
    "    prompt_template = \"\"\"\n",
    "You are a highly specialized AI that classifies locations.\n",
    "Your task is to analyze a list of features and provide the single most appropriate location category from the given list.\n",
    "The location categories are: {categories}.\n",
    "\n",
    "Example 1:\n",
    "Features: ['desk', 'computer', 'meeting']\n",
    "Output: office\n",
    "\n",
    "Example 2:\n",
    "Features: ['taste', 'menu', 'atmosphere', 'outdoor', 'indoor']\n",
    "Output: restaurant\n",
    "\n",
    "Instructions:\n",
    "Analyze the semantic meaning of the provided features and select the best matching category from the list. Your response must only contain the single word that is the category.\n",
    "\n",
    "Features: {features}\n",
    "Output:\n",
    "\"\"\"\n",
    "\n",
    "    # Format the prompt with the actual input features and categories.\n",
    "    features_str = ', '.join(f\"'{word}'\" for word in features)\n",
    "    categories_str = ', '.join(f\"'{cat}'\" for cat in categories)\n",
    "    prompt = prompt_template.format(features=f\"[{features_str}]\", categories=f\"[{categories_str}]\")\n",
    "\n",
    "    # Generate the text with specific parameters for concise output.\n",
    "    response = generator(\n",
    "        prompt, \n",
    "        max_length=256, \n",
    "        num_return_sequences=1, \n",
    "        do_sample=False,\n",
    "    )\n",
    "\n",
    "    # Extract and clean the generated word.\n",
    "    # T5 models return only the generated text, so no need to remove the prompt.\n",
    "    generated_text = response[0]['generated_text']\n",
    "    summary_word = generated_text.strip().lower()\n",
    "\n",
    "    # Ensure the output is one of the valid categories.\n",
    "    if summary_word in categories:\n",
    "        return summary_word\n",
    "    else:\n",
    "        return \"unidentified\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "86941e82-b6d2-4b68-983a-0d4aadf85bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.groupby(\"business_name\")[\"rating_category\"].agg(\" \".join).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "dd6fb14d-f912-42f4-bf30-9cc749165a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"rating_category\"] = df[\"rating_category\"].apply(lambda x: list(set(preprocess_text(x).split(\" \"))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "72272ba5-e3b1-4c41-a672-fde3f3fdfcf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    }
   ],
   "source": [
    "df[\"location\"] = df[\"rating_category\"].apply(lambda x: tag_location(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "982c2c8c-ef31-427a-bc85-fec3f7e5e353",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\"business_name\",\"location\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3857b5eb-0d8b-4ec6-a71e-8ec347a5e66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.merge(df, on=\"business_name\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b95c63d2-76bd-4062-bcc9-60ce145e0343",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_name</th>\n",
       "      <th>author_name</th>\n",
       "      <th>text</th>\n",
       "      <th>photo</th>\n",
       "      <th>rating</th>\n",
       "      <th>rating_category</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Haci'nin Yeri - Yigit Lokantasi</td>\n",
       "      <td>Gulsum Akar</td>\n",
       "      <td>We went to Marmaris with my wife for a holiday...</td>\n",
       "      <td>dataset/taste/hacinin_yeri_gulsum_akar.png</td>\n",
       "      <td>5</td>\n",
       "      <td>taste</td>\n",
       "      <td>restaurant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Haci'nin Yeri - Yigit Lokantasi</td>\n",
       "      <td>Oguzhan Cetin</td>\n",
       "      <td>During my holiday in Marmaris we ate here to f...</td>\n",
       "      <td>dataset/menu/hacinin_yeri_oguzhan_cetin.png</td>\n",
       "      <td>4</td>\n",
       "      <td>menu</td>\n",
       "      <td>restaurant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Haci'nin Yeri - Yigit Lokantasi</td>\n",
       "      <td>Yasin Kuyu</td>\n",
       "      <td>Prices are very affordable. The menu in the ph...</td>\n",
       "      <td>dataset/outdoor_atmosphere/hacinin_yeri_yasin_...</td>\n",
       "      <td>3</td>\n",
       "      <td>outdoor_atmosphere</td>\n",
       "      <td>restaurant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Haci'nin Yeri - Yigit Lokantasi</td>\n",
       "      <td>Orhan Kapu</td>\n",
       "      <td>Turkey's cheapest artisan restaurant and its f...</td>\n",
       "      <td>dataset/indoor_atmosphere/hacinin_yeri_orhan_k...</td>\n",
       "      <td>5</td>\n",
       "      <td>indoor_atmosphere</td>\n",
       "      <td>restaurant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Haci'nin Yeri - Yigit Lokantasi</td>\n",
       "      <td>Ozgur Sati</td>\n",
       "      <td>I don't know what you will look for in terms o...</td>\n",
       "      <td>dataset/menu/hacinin_yeri_ozgur_sati.png</td>\n",
       "      <td>3</td>\n",
       "      <td>menu</td>\n",
       "      <td>restaurant</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     business_name    author_name  \\\n",
       "0  Haci'nin Yeri - Yigit Lokantasi    Gulsum Akar   \n",
       "1  Haci'nin Yeri - Yigit Lokantasi  Oguzhan Cetin   \n",
       "2  Haci'nin Yeri - Yigit Lokantasi     Yasin Kuyu   \n",
       "3  Haci'nin Yeri - Yigit Lokantasi     Orhan Kapu   \n",
       "4  Haci'nin Yeri - Yigit Lokantasi     Ozgur Sati   \n",
       "\n",
       "                                                text  \\\n",
       "0  We went to Marmaris with my wife for a holiday...   \n",
       "1  During my holiday in Marmaris we ate here to f...   \n",
       "2  Prices are very affordable. The menu in the ph...   \n",
       "3  Turkey's cheapest artisan restaurant and its f...   \n",
       "4  I don't know what you will look for in terms o...   \n",
       "\n",
       "                                               photo  rating  \\\n",
       "0         dataset/taste/hacinin_yeri_gulsum_akar.png       5   \n",
       "1        dataset/menu/hacinin_yeri_oguzhan_cetin.png       4   \n",
       "2  dataset/outdoor_atmosphere/hacinin_yeri_yasin_...       3   \n",
       "3  dataset/indoor_atmosphere/hacinin_yeri_orhan_k...       5   \n",
       "4           dataset/menu/hacinin_yeri_ozgur_sati.png       3   \n",
       "\n",
       "      rating_category    location  \n",
       "0               taste  restaurant  \n",
       "1                menu  restaurant  \n",
       "2  outdoor_atmosphere  restaurant  \n",
       "3   indoor_atmosphere  restaurant  \n",
       "4                menu  restaurant  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "169d7f9c-151b-4b31-a91d-76a937647e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('data.csv', index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81979b1e-4bc1-4d14-8d29-f082db147f03",
   "metadata": {},
   "source": [
    "### Tagging Spam, Advertisment, Irrelevancy, and Rant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde33a17-8b58-4397-8609-e45df6b1d045",
   "metadata": {},
   "source": [
    "1. Irrelevancy: embedding of text are compared against embedding of location to find similarity score with 0.3 cut-off\n",
    "2. Spam: detection using spam keywords search\n",
    "3. Advertisement: detection using advertisement keywords search\n",
    "4. Rant: detection using rant keywords search\n",
    "   \n",
    "Review uality Score is information fusion of 4 tags above (1-(spam+advertisement+rant+irrelevancy)/4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0ddb11e0-546b-4f52-8db1-5a19d26fe58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Data\n",
    "data = pd.read_csv('data.csv',encoding='ISO-8859-1')\n",
    "data[\"text\"] = data[\"text\"].apply(lambda x: preprocess_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "56231388-920d-4c1d-ab03-6b517833e0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def tag_spam_adv_irrel_rant(review_text, location, similarity_threshold=0.3):\n",
    "    review_text = review_text.strip().lower()\n",
    "    location = location.strip().lower()\n",
    "\n",
    "    # 1. Relevancy using semantic similarity\n",
    "    review_emb = model.encode(review_text, convert_to_tensor=True)\n",
    "    location_emb = model.encode(location, convert_to_tensor=True)\n",
    "    similarity = util.pytorch_cos_sim(review_emb, location_emb).item()\n",
    "    irrelevant = 0 if similarity >= similarity_threshold else 1\n",
    "\n",
    "    # 2. Spam detection (doubled keywords)\n",
    "    spam_keywords = [\n",
    "        \"buy now\", \"free\", \"check out\", \"subscribe\",\n",
    "        \"click here\", \"limited time\", \"order now\", \"winner\"\n",
    "    ]\n",
    "    spam = 1 if any(word in review_text for word in spam_keywords) else 0\n",
    "\n",
    "    # 3. Advertisement detection (doubled keywords)\n",
    "    ad_keywords = [\n",
    "        \"promo\", \"deal\", \"offer\", \"visit my page\",\n",
    "        \"sale\", \"discount\", \"special offer\", \"advertisement\"\n",
    "    ]\n",
    "    advertisement = 1 if any(word in review_text for word in ad_keywords) else 0\n",
    "\n",
    "    # 4. Rant detection (doubled keywords)\n",
    "    rant_keywords = [\n",
    "        \"worst\", \"never coming back\", \"awful\", \"terrible\", \"horrible\",\n",
    "        \"disgusting\", \"poor service\", \"not recommended\", \"hate\"\n",
    "    ]\n",
    "    rant = 1 if any(word in review_text for word in rant_keywords) else 0\n",
    "\n",
    "    # 5. Fuse labels to compute quality score\n",
    "    labels = [irrelevant, spam, advertisement, rant]\n",
    "    quality_score = 1 - (sum(labels) / len(labels))  # 0 = worst, 1 = best\n",
    "\n",
    "    return {\n",
    "        \"irrelevant\": irrelevant,\n",
    "        \"spam\": spam,\n",
    "        \"advertisement\": advertisement,\n",
    "        \"rant\": rant,\n",
    "        \"quality_score\": quality_score\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9435bb98-69af-4d4b-8d0d-2fc80fffecc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply to DataFrame\n",
    "data[[\"irrelevant\", \"spam\", \"advertisement\", \"rant\", \"quality_score\"]] = data.apply(\n",
    "    lambda row: pd.Series(tag_spam_adv_irrel_rant(row[\"text\"], row[\"location\"])),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "315acbf2-fafb-46e8-8c35-a3e116bdced1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_name</th>\n",
       "      <th>author_name</th>\n",
       "      <th>text</th>\n",
       "      <th>photo</th>\n",
       "      <th>rating</th>\n",
       "      <th>rating_category</th>\n",
       "      <th>location</th>\n",
       "      <th>irrelevant</th>\n",
       "      <th>spam</th>\n",
       "      <th>advertisement</th>\n",
       "      <th>rant</th>\n",
       "      <th>quality_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Haci'nin Yeri - Yigit Lokantasi</td>\n",
       "      <td>Gulsum Akar</td>\n",
       "      <td>we went to marmaris with my wife for a holiday...</td>\n",
       "      <td>dataset/taste/hacinin_yeri_gulsum_akar.png</td>\n",
       "      <td>5</td>\n",
       "      <td>taste</td>\n",
       "      <td>restaurant</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Haci'nin Yeri - Yigit Lokantasi</td>\n",
       "      <td>Oguzhan Cetin</td>\n",
       "      <td>during my holiday in marmaris we ate here to f...</td>\n",
       "      <td>dataset/menu/hacinin_yeri_oguzhan_cetin.png</td>\n",
       "      <td>4</td>\n",
       "      <td>menu</td>\n",
       "      <td>restaurant</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Haci'nin Yeri - Yigit Lokantasi</td>\n",
       "      <td>Yasin Kuyu</td>\n",
       "      <td>prices are very affordable the menu in the pho...</td>\n",
       "      <td>dataset/outdoor_atmosphere/hacinin_yeri_yasin_...</td>\n",
       "      <td>3</td>\n",
       "      <td>outdoor_atmosphere</td>\n",
       "      <td>restaurant</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Haci'nin Yeri - Yigit Lokantasi</td>\n",
       "      <td>Orhan Kapu</td>\n",
       "      <td>turkey s cheapest artisan restaurant and its f...</td>\n",
       "      <td>dataset/indoor_atmosphere/hacinin_yeri_orhan_k...</td>\n",
       "      <td>5</td>\n",
       "      <td>indoor_atmosphere</td>\n",
       "      <td>restaurant</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Haci'nin Yeri - Yigit Lokantasi</td>\n",
       "      <td>Ozgur Sati</td>\n",
       "      <td>i don t know what you will look for in terms o...</td>\n",
       "      <td>dataset/menu/hacinin_yeri_ozgur_sati.png</td>\n",
       "      <td>3</td>\n",
       "      <td>menu</td>\n",
       "      <td>restaurant</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     business_name    author_name  \\\n",
       "0  Haci'nin Yeri - Yigit Lokantasi    Gulsum Akar   \n",
       "1  Haci'nin Yeri - Yigit Lokantasi  Oguzhan Cetin   \n",
       "2  Haci'nin Yeri - Yigit Lokantasi     Yasin Kuyu   \n",
       "3  Haci'nin Yeri - Yigit Lokantasi     Orhan Kapu   \n",
       "4  Haci'nin Yeri - Yigit Lokantasi     Ozgur Sati   \n",
       "\n",
       "                                                text  \\\n",
       "0  we went to marmaris with my wife for a holiday...   \n",
       "1  during my holiday in marmaris we ate here to f...   \n",
       "2  prices are very affordable the menu in the pho...   \n",
       "3  turkey s cheapest artisan restaurant and its f...   \n",
       "4  i don t know what you will look for in terms o...   \n",
       "\n",
       "                                               photo  rating  \\\n",
       "0         dataset/taste/hacinin_yeri_gulsum_akar.png       5   \n",
       "1        dataset/menu/hacinin_yeri_oguzhan_cetin.png       4   \n",
       "2  dataset/outdoor_atmosphere/hacinin_yeri_yasin_...       3   \n",
       "3  dataset/indoor_atmosphere/hacinin_yeri_orhan_k...       5   \n",
       "4           dataset/menu/hacinin_yeri_ozgur_sati.png       3   \n",
       "\n",
       "      rating_category    location  irrelevant  spam  advertisement  rant  \\\n",
       "0               taste  restaurant         0.0   0.0            0.0   0.0   \n",
       "1                menu  restaurant         0.0   0.0            0.0   0.0   \n",
       "2  outdoor_atmosphere  restaurant         0.0   0.0            0.0   0.0   \n",
       "3   indoor_atmosphere  restaurant         0.0   0.0            0.0   0.0   \n",
       "4                menu  restaurant         0.0   0.0            0.0   0.0   \n",
       "\n",
       "   quality_score  \n",
       "0            1.0  \n",
       "1            1.0  \n",
       "2            1.0  \n",
       "3            1.0  \n",
       "4            1.0  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d7b8769c-5b0c-49c8-829a-8d6ff62be422",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('data.csv', index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14382951-8b73-4ca9-b484-7783848c4843",
   "metadata": {},
   "source": [
    "## Part 2: Policy Enforcement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c11679-73fa-465c-a863-feb24c944c8f",
   "metadata": {},
   "source": [
    "A function to join the tagged(labelled) data with policy tags - joining two dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ceae4b4f-cfa7-4f6e-8642-0c0deb685ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to enforce user policy on reviews data\n",
    "\n",
    "def enforce_policies(data_file: str, policy_file: str = \"policies.json\", output_file: str = \"data_w_policy.csv\"):\n",
    "    \"\"\"\n",
    "    Enforces policies on review data by merging review labels with policy definitions.\n",
    "\n",
    "    Args:\n",
    "        data_file (str): Path to input review data CSV file.\n",
    "        policy_file (str): Path to policies JSON file (default: 'policies.json').\n",
    "        output_file (str): Path to save the merged CSV file (default: 'data_w_policy.csv').\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The merged DataFrame with policies applied.\n",
    "    \"\"\"\n",
    "    # Load review data\n",
    "    data = pd.read_csv(data_file, encoding=\"ISO-8859-1\")\n",
    "\n",
    "    # Load policies\n",
    "    with open(policy_file, \"r\") as f:\n",
    "        policies_dict = json.load(f)\n",
    "\n",
    "    # Convert list of dicts to DataFrame\n",
    "    policy_df = pd.DataFrame(policies_dict)\n",
    "\n",
    "    # Merge on label columns\n",
    "    data_w_policy = data.merge(\n",
    "        policy_df,\n",
    "        how=\"left\",\n",
    "        on=[\"advertisement\", \"irrelevant\", \"rant\"]\n",
    "    )\n",
    "\n",
    "    # Replace NaNs with None for JSON compatibility\n",
    "    data_w_policy = data_w_policy.replace({np.nan: None})\n",
    "\n",
    "    # Save to CSV\n",
    "    data_w_policy.to_csv(output_file, index=False, encoding=\"utf-8\")\n",
    "\n",
    "    return data_w_policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9fc8ee87-d5aa-4728-91b8-50812a6f2c75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_name</th>\n",
       "      <th>author_name</th>\n",
       "      <th>text</th>\n",
       "      <th>photo</th>\n",
       "      <th>rating</th>\n",
       "      <th>rating_category</th>\n",
       "      <th>location</th>\n",
       "      <th>irrelevant</th>\n",
       "      <th>spam</th>\n",
       "      <th>advertisement</th>\n",
       "      <th>rant</th>\n",
       "      <th>quality_score</th>\n",
       "      <th>policy_type</th>\n",
       "      <th>policy_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Haci'nin Yeri - Yigit Lokantasi</td>\n",
       "      <td>Gulsum Akar</td>\n",
       "      <td>we went to marmaris with my wife for a holiday...</td>\n",
       "      <td>dataset/taste/hacinin_yeri_gulsum_akar.png</td>\n",
       "      <td>5</td>\n",
       "      <td>taste</td>\n",
       "      <td>restaurant</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Haci'nin Yeri - Yigit Lokantasi</td>\n",
       "      <td>Oguzhan Cetin</td>\n",
       "      <td>during my holiday in marmaris we ate here to f...</td>\n",
       "      <td>dataset/menu/hacinin_yeri_oguzhan_cetin.png</td>\n",
       "      <td>4</td>\n",
       "      <td>menu</td>\n",
       "      <td>restaurant</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Haci'nin Yeri - Yigit Lokantasi</td>\n",
       "      <td>Yasin Kuyu</td>\n",
       "      <td>prices are very affordable the menu in the pho...</td>\n",
       "      <td>dataset/outdoor_atmosphere/hacinin_yeri_yasin_...</td>\n",
       "      <td>3</td>\n",
       "      <td>outdoor_atmosphere</td>\n",
       "      <td>restaurant</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Haci'nin Yeri - Yigit Lokantasi</td>\n",
       "      <td>Orhan Kapu</td>\n",
       "      <td>turkey s cheapest artisan restaurant and its f...</td>\n",
       "      <td>dataset/indoor_atmosphere/hacinin_yeri_orhan_k...</td>\n",
       "      <td>5</td>\n",
       "      <td>indoor_atmosphere</td>\n",
       "      <td>restaurant</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Haci'nin Yeri - Yigit Lokantasi</td>\n",
       "      <td>Ozgur Sati</td>\n",
       "      <td>i don t know what you will look for in terms o...</td>\n",
       "      <td>dataset/menu/hacinin_yeri_ozgur_sati.png</td>\n",
       "      <td>3</td>\n",
       "      <td>menu</td>\n",
       "      <td>restaurant</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     business_name    author_name  \\\n",
       "0  Haci'nin Yeri - Yigit Lokantasi    Gulsum Akar   \n",
       "1  Haci'nin Yeri - Yigit Lokantasi  Oguzhan Cetin   \n",
       "2  Haci'nin Yeri - Yigit Lokantasi     Yasin Kuyu   \n",
       "3  Haci'nin Yeri - Yigit Lokantasi     Orhan Kapu   \n",
       "4  Haci'nin Yeri - Yigit Lokantasi     Ozgur Sati   \n",
       "\n",
       "                                                text  \\\n",
       "0  we went to marmaris with my wife for a holiday...   \n",
       "1  during my holiday in marmaris we ate here to f...   \n",
       "2  prices are very affordable the menu in the pho...   \n",
       "3  turkey s cheapest artisan restaurant and its f...   \n",
       "4  i don t know what you will look for in terms o...   \n",
       "\n",
       "                                               photo  rating  \\\n",
       "0         dataset/taste/hacinin_yeri_gulsum_akar.png       5   \n",
       "1        dataset/menu/hacinin_yeri_oguzhan_cetin.png       4   \n",
       "2  dataset/outdoor_atmosphere/hacinin_yeri_yasin_...       3   \n",
       "3  dataset/indoor_atmosphere/hacinin_yeri_orhan_k...       5   \n",
       "4           dataset/menu/hacinin_yeri_ozgur_sati.png       3   \n",
       "\n",
       "      rating_category    location  irrelevant  spam  advertisement  rant  \\\n",
       "0               taste  restaurant         0.0   0.0            0.0   0.0   \n",
       "1                menu  restaurant         0.0   0.0            0.0   0.0   \n",
       "2  outdoor_atmosphere  restaurant         0.0   0.0            0.0   0.0   \n",
       "3   indoor_atmosphere  restaurant         0.0   0.0            0.0   0.0   \n",
       "4                menu  restaurant         0.0   0.0            0.0   0.0   \n",
       "\n",
       "   quality_score policy_type policy_description  \n",
       "0            1.0        None               None  \n",
       "1            1.0        None               None  \n",
       "2            1.0        None               None  \n",
       "3            1.0        None               None  \n",
       "4            1.0        None               None  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enforce_policies(\"data.csv\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9549b481-d001-4068-b6f1-7d76cf577dc6",
   "metadata": {},
   "source": [
    "## Part 3: Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a46296d2-0991-4e64-97df-925a60936cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['business_name', 'author_name', 'text', 'photo', 'rating', 'rating_category', 'location', 'irrelevant', 'spam', 'advertisement', 'rant', 'quality_score']\n"
     ]
    }
   ],
   "source": [
    "#Read Data From File\n",
    "data = pd.read_csv('data.csv',encoding='ISO-8859-1')\n",
    "cols = list(data.columns)\n",
    "print(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "63f261b2-a5a4-4012-9885-e092ef99b968",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check The Data Balance\n",
    "input_cols = ['text']\n",
    "output_cols = ['spam', 'advertisement', 'rant', 'irrelevant','quality_score']\n",
    "categorical_cols = ['spam','advertisement','rant','irrelevant']\n",
    "numerical_cols = ['quality_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "62e0b577-f118-4fe0-837f-e377467ea49a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     spam  advertisement  rant  irrelevant\n",
      "0.0  1099           1088  1086         486\n",
      "1.0     1             12    14         614\n"
     ]
    }
   ],
   "source": [
    "#Check the class balance\n",
    "df = data[categorical_cols]\n",
    "counts = pd.DataFrame({col: df[col].value_counts() for col in df.columns}).fillna(0).astype(int)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c4caf6cf-f077-4de3-b399-f86a29994241",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from skmultilearn.model_selection import iterative_train_test_split\n",
    "import numpy as np\n",
    "\n",
    "def balanced_split_dataframe(df, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Splits a pandas DataFrame into balanced train and test sets \n",
    "    for multi-label classification.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe with 'text' and label columns.\n",
    "        test_size (float): Fraction of data to use for testing.\n",
    "        random_state (int): Random seed for reproducibility.\n",
    "    \n",
    "    Returns:\n",
    "        X_train, X_test, y_train, y_test\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Define input and output columns\n",
    "    input_col = \"text\"\n",
    "    output_cols = [\"spam\", \"advertisement\", \"rant\", \"irrelevant\", \"quality_score\"]\n",
    "    \n",
    "    # Extract features (X) and labels (y)\n",
    "    X = df[[input_col]].values\n",
    "    y = df[output_cols].values\n",
    "    \n",
    "    # Iterative stratification split\n",
    "    X_train, y_train, X_test, y_test = iterative_train_test_split(\n",
    "        X, y, test_size=test_size\n",
    "    )\n",
    "    \n",
    "    # Convert back to DataFrame\n",
    "    X_train = pd.DataFrame(X_train, columns=[input_col])\n",
    "    X_test = pd.DataFrame(X_test, columns=[input_col])\n",
    "    y_train = pd.DataFrame(y_train, columns=output_cols)\n",
    "    y_test = pd.DataFrame(y_test, columns=output_cols)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a14902b2-5922-4689-a656-a1cc2e7afe8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data:\n",
      "     spam  advertisement  rant  irrelevant\n",
      "0.0   879            872   869         389\n",
      "1.0     1              8    11         491\n",
      "Test Data:\n",
      "     spam  advertisement  rant  irrelevant\n",
      "0.0   220            216   217          97\n",
      "1.0     0              4     3         123\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = balanced_split_dataframe(data)\n",
    "df = y_train[categorical_cols]\n",
    "counts = pd.DataFrame({col: df[col].value_counts() for col in df.columns}).fillna(0).astype(int)\n",
    "print(\"Train Data:\")\n",
    "print(counts)\n",
    "df = y_test[categorical_cols]\n",
    "counts = pd.DataFrame({col: df[col].value_counts() for col in df.columns}).fillna(0).astype(int)\n",
    "print(\"Test Data:\")\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "bcfc4ee8-0f15-4e5a-b12b-8fc73626b8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to train with ensemble method and test the labelled data.\n",
    "\n",
    "def train_and_test_ensemble(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Train a multi-label text classifier with an ensemble model \n",
    "    and evaluate precision, recall, f1 per class.\n",
    "    \n",
    "    Args:\n",
    "        X_train (pd.DataFrame): Training input with 'text' column\n",
    "        X_test (pd.DataFrame): Test input with 'text' column\n",
    "        y_train (pd.DataFrame): Training labels (spam, advertisement, rant, irrelevant)\n",
    "        y_test (pd.DataFrame): Test labels\n",
    "    \n",
    "    Returns:\n",
    "        metrics_df (pd.DataFrame): Table of precision, recall, f1 for each class\n",
    "    \"\"\"\n",
    "    \n",
    "    label_cols = [\"spam\", \"advertisement\", \"rant\", \"irrelevant\"]\n",
    "    \n",
    "    # Vectorize text\n",
    "    vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1,2))\n",
    "    X_train_tfidf = vectorizer.fit_transform(X_train[\"text\"])\n",
    "    X_test_tfidf = vectorizer.transform(X_test[\"text\"])\n",
    "    \n",
    "    # Ensemble classifier (Random Forest)\n",
    "    base_clf = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)\n",
    "    clf = MultiOutputClassifier(base_clf)\n",
    "    clf.fit(X_train_tfidf, y_train[label_cols])\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = clf.predict(X_test_tfidf)\n",
    "    \n",
    "    # Collect metrics\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        y_test[label_cols], y_pred, average=None, labels=range(len(label_cols))\n",
    "    )\n",
    "\n",
    "    #Predicted Quality Score\n",
    "    quality_score_pred = list(y_pred.mean(axis=1))\n",
    "\n",
    "    \n",
    "    # Extract true quality_score from y_test\n",
    "    quality_score_true = y_test['quality_score'].tolist()\n",
    "    \n",
    "    correct_predictions = sum(1 for true, pred in zip(quality_score_true, quality_score_pred) if true == pred)\n",
    "    accuracy = correct_predictions / len(quality_score_true)\n",
    "    \n",
    "    print(\"Quality Score Prediction Accuracy:\", accuracy)\n",
    "    print(\"\\nPer-Class Metrics:\")\n",
    "    \n",
    "    # Convert to DataFrame (tabular format)\n",
    "    metrics_df = pd.DataFrame({\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1\n",
    "    }, index=label_cols)\n",
    "    \n",
    "    return metrics_df, accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4257ac-35e9-45dc-823c-94bd6ecc1edd",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "1. Accuracy of Quality Score predicted\n",
    "2. Precision, Recall, F1 scores for the different text classification viz., spam, advertisement, rant, and irrelevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "35f0b829-955a-4cce-b9b1-aa3a483ffeae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quality Score Prediction Accuracy: 0.0\n",
      "\n",
      "Per-Class Metrics:\n",
      "               precision    recall        f1\n",
      "spam            0.000000  0.000000  0.000000\n",
      "advertisement   0.000000  0.000000  0.000000\n",
      "rant            0.000000  0.000000  0.000000\n",
      "irrelevant      0.710145  0.796748  0.750958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunal/.conda/envs/av/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kunal/.conda/envs/av/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kunal/.conda/envs/av/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "metrics, quality_acc = train_and_test_ensemble(X_train, X_test, y_train, y_test)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fc266b-c937-43b4-a0ab-39dd2b2938c8",
   "metadata": {},
   "source": [
    "Conclusion:\n",
    "Given the time constraints, we could only show one of the ways to solve the given problem. \n",
    "The test results are far from reasonable because of two reasons:\n",
    "1. Poor Data Tagging: we admit the shortcomings in the data tagging. Finding a refined method to tag the data would have required more time, something we could not afford to provide.\n",
    "2. Imbalanced Data: the results shown are tested on limited data. Finding more data points through web scraping could have helped, something we did not try given limited available time. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
